% !TeX spellcheck = en_GB
\def\ChapterTitle{}

\chapter{Multi-Domain Trust Assessment in Collaborative Marine \glspl{manet}}
\label{ch:multi_domain}
\lhead{Chapter \thechapter. \emph{\nameref{ch:multi_domain}}}

\section{Introduction}

In this chapter, a multi-domain trust management framework (MD-TMF) is demonstrated in collaborative marine \glspl{manet}
A methodology is demonstrated that applies Grey Sequence operations and Grey Generators to provide continuous trust assessment in a sparse, asynchronous metric space across multiple domains of trust.
By utilising information from multiple domains, it is demonstrated that trust assessment can be more accurate and consistent in identifying misbehaviour than in single-domain assessment.
Further, a methodology for assessing the usefulness of individual metrics in this cross-domain space is demonstrates, allowing for the elimination of redundant metrics, simplifying the runtime assessment process.

\section{Initial Optimisation of Multi-Domain Trust with Predefined Domains}

A key question in this chapter is to assess the advantages and disadvantages of utilising trust from across domains. 
This includes a secondary question as to how trust assessments from these domains are most effectively combined. 

It is important to clarify what is meant by ``effective'' in this case; the ``effectiveness'' of any trust assessment framework is taken as consisting of several parts.

\begin{enumerate}
  \item the \emph{accuracy} of detection and identification of a particular misbehaviour
  \item the \emph{timeliness} of such detections
  \item the \emph{complexity} of such analysis, including any specific training required
  \item the \emph{commonality} of the results of any detections between perspectives (also termed ``isomorphism'' of results)
\end{enumerate}



\subsection{Communications Trust Metrics}

The metric vector is constructed using those trust metrics that are applicable to the marine environment from \cite{Guo2012}, as the simulated marine acoustic modem stack does not operate on the same tiered data-rate approach as used in the 802.11 stack, the data rate metric was not included. Remaining metrics are; Delay, Received and Transmitted power, Throughput ($S$), Offered Load ($G$) and \gls{plr}.

Thus, the metric vector used for communications-trust assessment is;

\begin{equation}
  X_{comms}=\{D, P_{RX}, P_{TX}, S, G, PLR\}
  \label{eq:comms_vector}
\end{equation}

\subsection{Physical Trust Metrics}

From \autoref{ch:comms_trust}; Three physical metrics are selected to encompass the relative distributions and activities of nodes within the network; \gls{indd}, \gls{inhd}, and Node Speed. These metrics encapsulate the relative distributions of position and velocity within the fleet, optimising for the detection of outlying or deviant behaviour within the fleet.

Conceptually, \gls{indd} is a measure of the average spacing of an observed node with respect to its neighbours. \gls{inhd} is a similar approach with respect to node orientation.

\begin{align}
  INDD_{i,j} &= \frac{|P_j - \sum_x \frac{P_x}{N}|}{\frac{1}{N}\sum_x \sum_y{|P_x - P_y| (\forall x \neq y)}}\\
  INHD_{i,j} &= \hat{v} \vert v= V_j - \sum_x{\frac{V_x}{N}}\\
  V_{i,j} &= |V_j|
\end{align}

Thus, the metric vector used for physical-trust assessment is;

\begin{equation}
  X_{phy}=\{\text{INDD}, \text{INHD}, V\}
  \label{eq:phys:vector}
\end{equation}


\subsection{Cross Domain Trust Metrics}
This simplest possible combination is a vector concatenation across domain metric vectors; in this case; 

\begin{equation}
  X_{merge} =  (X_{comms}|X_{phy}) = \{D, P_{RX}, P_{TX}, S, G, PLR, \text{INDD}, \text{INHD}, V\}
  \label{eq:phys:vector}
\end{equation}


\subsection{Metric Weight Analysis Scheme}

From \eqref{eq:metric_weighting}, the final trust values arrived at are dependent on metric values, the weights assigned to each metric, and the structure of the $g$, $b$ comparison vectors.

This permits the assessment of the significance of different metrics in the detection and identification of different behaviours. 
The primary aspects of a (mis)behaviour can be detected and assessed by comparing a weighted trust assessment against the deviation from a ``fair'' result set using the same weight, i.e.\ we are interested in the weight schemes that create the largest difference between fair and misbehaving cases.

For a metric weight vector $H$, where the metric $m_j$ is emphasised as being twice as important as the other metrics, an initial weighting vector $H'=[h_i\cdots h_M]$ is formed such that $h_i = 1 \forall i \ne j; h_j=2$. That vector $H'$ is then scaled such that $\sum H = 1$ by $H= \frac{H'}{\sum H'}$.

The construction of the $g$ and $b$ vectors from~\eqref{eq:grc} depends on the particular metric, e.g. Throughput ($S$) on a link is assumed to be positively correlated to trustworthiness and so follows the default construction ($g(S) \mapsto \max, b(S) \mapsto \min$), whereas in the case of a metric such as delay, this relationship is inverted, i.e.\ longer delays indicate less trustworthy activity ($g(D) \mapsto \min, b(D) \mapsto \max$).
This inversion relationship (i.e.\ those with the construction $g(x) \mapsto \min, b(x) \mapsto \max$) is signified by a negative weight.

In complex environments, the relationship between metrics trustworthiness correlations is not always as obvious as the throughput / delay examples.
This phenomenon was mentioned by \citet{Guo2012}, but was manually configured for each metric for each behaviour and no analytical method for quantitatively establishing such relationships has been presented since.

With the nine selected metrics from across communications and physical behaviours, we can explore this metric space by varying the weights associated with each metric, and choose to emphasise across three levels; i.e.\ metrics can be ignored or over-emphasised. Naively this results in $3^9 = 19683$ combinations, however as these weights are being normalised, redundant duplicates can be eliminated, e.g. $[0,0,0,0,1,0,0,0,0] \equiv [0,0,0,0,2,0,0,0,0]$ leaving 18661 unique weights for analysis.

To assess the performance of a given weight combination (i.e.\ an optimisation factor), we are initially interested in the metric weight vector that consistently provides the largest deviation in the final trust value $T$ across the cohort, i.e.\ producing the most clear detection of a node misbehaving in that particular fashion.
This is approached as an inverse outlier filtering problem, and the range outside a $\pm\sigma$ envelope compared to the equivalent weighting in a known ``fair'' behaviour is selected to assess detection (or comparing to other misbehaviours to assess discrimination).
See~\autoref{sec:metric_weighting}.
Note that at this point we establish ``signatures'' of different behaviours rather than optimal detection weights.

We apply a Random Forest regression \cite{Breiman2001} to assess the relative importance of the selected metrics on relative detectability of malicious behaviour. 
Random Forest accomplishes this by generating a large number of random regression trees and prune these trees based on how accurate they are in correctly matching the input data.
In this case that data is the deviation in trust observed ($\Delta T$) between a two behaviours, i.e. maximising the ability to tell the difference between two given behaviours (i.e.\ ``Fair'' and ``Malicious'').
A major advantage of Random Forest in this case is that by walking the most successful regression trees, we can acquire an already normalised maximal activation weight for the particular behaviour comparison being tested.

After establishing the importance of weights in particular behaviours, a final weight is arrived at by algorithmically those few metrics that are important, rather than having to further explore the computationally expensive weight-space.

Using this approach, the results of these simulations can be explored, condensing the multi-dimensional problem (target / observer / behaviour / metric / time) down to a more manageable level for analysis.

\subsection{Significance Analysis}

First the results of the Random Forest regression assessment are discussed; Figs~\ref{fig:comms_feature_extraction} and~\ref{fig:phys_feature_extraction}, show the resultant feature extraction signatures for Comms-only and Physical-only metric selections respectively, and in Fig~\ref{fig:multi_feature_extraction}, these metric spaces are brought together and reassessed.

In both single-domain cases, there are clear ``signatures'' in misbehaviours that don't directly target that domain ($P_{RX}$ in the Physical Shadow and Slowcoach behaviours in Fig~\ref{fig:comms_feature_extraction} and $INDD$ in the Selfish Target Selection behaviour in Fig~\ref{fig:phys_feature_extraction}).
This inter-domain activity is to be expected in \glspl{manet} in general, where the physical reality of the network (i.e.\ distance between nodes) directly impacts the behaviour of the logical communications network (i.e.\ delay between nodes), and is a useful characteristic for differentiating potential misbehaviours.\todo{ADD: Come back to this and talk about redundancy after subset analysis}



% These figures are the output of test_Thesis_Diagrams.test????Relevance
\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{comms_metric_trust_relevance}
  \caption{Plot of Communications Metric Feature Extraction ($X_{comms}$)}
  \label{fig:comms_feature_extraction}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{phys_metric_trust_relevance}
  \caption{Plot of Physical Metric Feature Extraction ($X_{phys}$)}
  \label{fig:phys_feature_extraction}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{full_metric_trust_relevance}
  \caption{Multi Domain  Metric Features Extraction ($X_{merge}$)}
  \label{fig:multi_feature_extraction}
\end{figure}

\begin{table}
  \centering
  \caption{Multi Domain Metric Feature Correlation ($X_{merge}$)}
  \input{Figures/input/full_metric_correlations.tex}
  \label{tab:full_metric_correlations}
\end{table}



\subsection{Weight Assessment}\label{sec:weight_assessment}

From this significance information, a ``estimated'' signature for each behaviour can be inferred, which can then be fed back into the assessment framework. 
The aim of this iteration is to minimise the number of weight permutations required to come to a conclusion about the behaviour under observation. 

However, these approximated signatures have no information regarding the ``sign'' of the  $g$,$b$ comparison vectors from \eqref{eq:metric_weighting}, i.e.\ there is no hint as to whether the relationship is $g(x) \mapsto \max, b(x) \mapsto \min$ or $g(x) \mapsto \min, b(x) \mapsto \max$)  

One option would be to go back to the regression point and expand the combination options to include negative values, signifying inverted $g,b$ relationships, however this is combinatorially explosive.\footnote{The current version of this analysis uses three metric weights; ignored, standard, emphasised, giving $3^9 = 19683$ combinations. Expanding this to include inverted standard and inverted emphasised weights would raise that to $5^9 = 1.9\times 10^6$}
Instead, the ``significance'' weight is permuted against it's possible combinations of ``flips'', i.e.\ for $X_s=[0.3,0.4,0.01,0.02,0.27]$ could also be $X_s^p=[0.3,-0.4,0.01,0.02,0.27]$ and so on. 
This sign permutation is filtered based on a threshold value ($0.01$), so for all indices below that threshold will not be permuted on, halving the number of combinations required for each indices eliminated.
This reduces the number of additional assessments required from $1.9\times 10^6$ to approximately 500 (when applied to all nine metrics).

The best of these permutations is selected to both maximise the (correct) deviation between each nodes trust perspectives and to minimise the trust value reported for the misbehaving nodes; $\Delta T \to \max^+$ (\autoref{eq:delta_t}, results summarised in \autoref{tab:domain_deltas}).
Additionally, a ``False Positive'' assessment, $\Delta T^-$ \autoref{eq:delta_t_minus} is shown in \autoref{tab:domain_deltas_minus} which encapsuates the average false positive selection rate.

\begin{align}
  \Delta T_{ix} &= \frac{\sum_{j\neq x}\left( \overline{T_{i,j}}^{\forall t}\right)}{N-1} - \overline{T_{i,x}}^{\forall t} \label{eq:delta_t}\\
  \Delta T_{ix}^- &= \frac{\sum_{j\neq x} \Delta T_{ij}}{N-1} - \overline{\Delta T_{i,x}}^{\forall t} \label{eq:delta_t_minus} 
\end{align}

Where $i$ is a given observer, $x$ is the known misbehaving node, $\overline{T_{i,j}}^{\forall t}$ is the average weighted trust assessment of node $j$ observed by node $i$ across time and $N$ is the number of nodes.

Conceptually, $\Delta T_{ix}$ represents the ``Distrust'' of the target node $x$, as the difference in trust value from $0\to1$, the higher the better. 
$\Delta T_{ix}^-$ represents the average $\Delta T_{ij}$ for all other nodes, representing the likelihood of another node being as highly distrusted as $x$, where positive values indicate that $x$ is not the obvious outlier, negative values indicate that $x$ is a very clear outlier, and near-zero values indicate a difficulty in selection of any outlier from the cohort.

\todo{FIX: Could do with a conceptual graphic showing what these look like, although it'd be messy as all hell}

\todo{ADD: Could also do with a investigation into the deviation of T's; so far most of this analysis averages everything, which is almost certainly not the best approach alone}

The ``best'' weight permutations, as shown in \autoref{tab:optimised_weights}, are applied to untrained datasets for these results.

An exemplar subset of the results is shows in Figs~\ref{fig:comms_time_mpc}-~\ref{fig:full_time_slowcoach}, with the ``misbehaving node'' highlighted with heavier lines, with any observations about the rest of the cohort faded and dashed. For each node assessment, the mean for that assessment over that time period is also included as a solid / dashed line respectively for clarity.

The most intuiativly ``Communications'' behaviour, \gls{mpc}, scores comfortably in the 90th percentile range in both Communications Domain (\autoref{fig:comms_time_mpc}) and Full Domain (\autoref{fig:full_time_mpc})  trust assessments. As seen in \autoref{tab:optimised_weights}, both the ``Full'' and ``Comms'' metric optimsations heavily weigh $P_{TX}$, and as this is the metric directly modified by the misbehaviour, it is expected that this is easily discernable using these domain weights.
However when this communications information is unavailable, as is the case in the use of Physical Domain metrics alone in \autoref{fig:phys_time_mpc}, the misbehaving node (Alfa) is completely undiscernible compared to the other nodes, with all nodes in the cohort tending to a trust value of $0.5$.
How this discernibility would fare under varying emphasis of behaviours is an open question\todo{FUT: Answer what happens when you vary MPC power variation}

Under the most ``subtle'' behaviour; \gls{sts}, where no direct metric is being modified in operation, but where the behaviour is effectively in the ``Application layer'' of the networking stack, the picture is far more murky. 
Comparing Figs~\ref{fig:comms_time_sts} and~\ref{fig:full_time_sts}, while there is a reasonable dip in the misbehavor's trust assessment, the high level of variance across the cohort is such that this ``mistrust'' triggering is neither consistent or obvious. 
From \autoref{tab:optimised_weights}, the metric of import is $G$, the Offered Load on the network, and given it's negative weighting, this matches the expectation that the node doing ``less than it's fair share'' is potentially misbehaving. 
Unfortunately this is the case across the \gls{sts} responses, where in Table~\ref{tab:domain_deltas} we have summarized out general results, \gls{sts} has by far and away the lowest average $\Delta T$ in all domains. 
Interestingly however is the observation that Comms-only trust performs slightly better than Full trust weighting.

Referring to Figs~\ref{fig:comms_feature_extraction} and~\autoref{fig:multi_feature_extraction}, it's clear that the offered load ($G$) is the almost singular feature of this behaviour, due to it's almost completely logical behaviour that is only loosely coupled to the state of the environment. 
The massive emphasis placed on load could only be diminished by putting it together in a larger ensemble.
In Figs~\ref{fig:comms_time_shadow} and~\ref{fig:full_time_shadow}, the misbehaving node is much more obvious than in \gls{sts}, which is moderately surprising for a physically-focused behaviour. Further, there is a roughly 20\% improvement when incorporating the full metric space.

From Table~\ref{tab:domain_deltas}, the Shadow behavior is the most consistently detectable behaviour across selected metric domains. 


See \autoref{sec:apx_mean_targeting_malicious} for a full collection of graphs showing the comparison of the malicious nodes trust value against the instantenous mean of the remaining cohort.
See \autoref{sec:apx_targeting_non_malicious} for a full collection of graphs showing the comparison of non-malicious nodes trust value against the individual values of the remaining cohort.
See \autoref{sec:apx_mean_targeting_non_malicious} for a full collection of graphs showing the comparison of non-malicious nodes trust value against the instantenous mean of the remaining cohort.


% These figures are the output of test_Thesis_Diagrams.test0ValidationBestPlots
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_comms_run_time_MPC}
  \caption{\gls{mpc} Comms Metric Trust (showing cohort trust assessments)}
  \label{fig:comms_time_mpc}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_phys_run_time_MPC}
  \caption{\gls{mpc} Physical Metric Trust (showing cohort trust assessments)}
  \label{fig:phys_time_mpc}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_full_run_time_MPC}
  \caption{\gls{mpc} Full Metric Trust (showing cohort trust assessments)}
  \label{fig:full_time_mpc}
\end{figure}


\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_comms_run_time_STS}
  \caption{\gls{sts} Comms Metric Trust (showing cohort trust assessments)}
  \label{fig:comms_time_sts}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_phys_run_time_STS}
  \caption{\gls{sts} Physical Metric Trust (showing cohort trust assessments)}
  \label{fig:phys_time_sts}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_full_run_time_STS}
  \caption{\gls{sts} Full Metric Trust (showing cohort trust assessments)}
  \label{fig:full_time_sts}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_comms_run_time_Shadow}
  \caption{Shadow Comms Metric Trust (showing cohort trust assessments)}
  \label{fig:comms_time_shadow}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_phys_run_time_Shadow}
  \caption{Shadow Physical Metric Trust (showing cohort trust assessments)}
  \label{fig:phys_time_shadow}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_full_run_time_Shadow}
  \caption{Shadow Full Metric Trust (showing cohort trust assessments)}
  \label{fig:full_time_shadow}
\end{figure}



\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_comms_run_time_SlowCoach}
  \caption{SlowCoach Comms Metric Trust (showing cohort trust assessments)}
  \label{fig:comms_time_slowcoach}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_phys_run_time_SlowCoach}
  \caption{SlowCoach Physical Metric Trust (showing cohort trust assessments)}
  \label{fig:phys_time_slowcoach}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{best_full_run_time_SlowCoach}
  \caption{SlowCoach Full Metric Trust (showing cohort trust assessments)}
  \label{fig:full_time_slowcoach}
\end{figure}

% These tables are the output of test_Thesis_Diagrams.ThesisDiagrams#test0ValidationBestPlots
\begin{table}
  \centering
  \caption{$\Delta T$ across domains and ``proposed'' behaviours targeting known misbehaving node}
  \input{Figures/input/domain_deltas.tex}
  \label{tab:domain_deltas}
\end{table}

\begin{table}
  \centering
  \caption{$\Delta T^-$ False Positive assessments across domains and ``proposed'' behaviours across non-misbehaving nodes}
  \input{Figures/input/domain_deltas_minus.tex}
  \label{tab:domain_deltas_minus}
\end{table}
\todo{FIX CODE: Haven't worked out a clever way of automatically generating both the basic domain and alternate domain texes easily}
\todo{FIX CODE: These tables need changed from being ``delta T'' to ``Probability of detection by an idiotic classifier}

\begin{landscape}
  \begin{table}
    \centering
    \caption{Optimised metric vector weights per domain trained upon and behaviour targeted}
    \input{Figures/input/optimised_weights.tex}
    \label{tab:optimised_weights}
  \end{table}
\end{landscape}


\begin{figure}[h]
  \centering
  \begin{tikzpicture}[-latex,
    comms/.style={text=blue, draw=blue, rounded corners=.8ex},
    phys/.style={text=red, draw=red, rounded corners=.8ex}]
    % Start off with the line
    %\draw [<-|][comms, draw=blue, very thick] (0,1) -- (5,1); 
    %\draw [|->][phys, draw=red, very thick] (5,1) -- (10,1); 
    \path[left color=blue,right color=red]
    (0,0.95) rectangle +(10,2.5pt);

    % Baseline labels
    \node[comms, fill=blue, fill opacity=0.2, text opacity=1] at (-1,1) {Comms.};
    \node[phys, fill=red, fill opacity=0.2, text opacity=1] at (11,1) {Physical};

    % Metric Labels
    \node[comms] at (7,2.2) {$D$};
    \node[comms] at (9,2.2) {$P_{RX}$};
    \node[comms] at (1,1.4) {$P_{TX}$};
    \node[comms] at (2.5,1.4) {$S$};
    \node[comms] at (3.3,1.4) {$G$};
    \node[comms] at (2,2.2) {$PLR$};
    \node[phys] at (5,2.2) {$INDD$};
    \node[phys] at (7,1.4) {$INHD$};
    \node[phys] at (9,1.4) {$Speed$};

    \node[rotate=90, align=center] at (-1, 3) {Metric\\Clustering};
    \node[rotate=90, align=center] at (-1, -1.5) {Apparent Domain\\of Misbehaviour};
    \node[comms] at (1,-1.4) {$MPC$};
    \node[comms] at (2.5,-0.8) {$STS$};
    \node[phys] at (6.5,-1.8) {$Shadow$};
    \node[phys] at (8.7,-1) {$SlowCoach$};

    % Draw approx subsets
    \draw[comms] (2.9,1.9) ellipse (3.3 and 1.4);
    \draw[phys] (7.2,1.9) ellipse (3.3 and 1.4);

    \node[text=red] at (7.2,3.8) {Phys. Alt.};
    \node[text=blue] at (2.9,3.8) {Comms. Alt.};

    % Misc
    \draw[-|][dotted] (5,5) -- (5,1);
    \draw[-|][dotted] (5,-3) -- (5,1);


  \end{tikzpicture}
  \caption{Assumptions made about the relevant domains of impact / detectibility of misbehaviours, and domain relevance of metrics, may not be optimal}
  \label{fig:alternate_domain_diag}
\end{figure}

\section{Optimal Metric Subset Analysis}
\todo{ADD: Subset Analysis}


\section{Conclusion}
In this chapter we demonstrate that in harsh environments, multi-domain trust assessment can perform better on average than single-domain counterparts, both in terms of robustness and sensitivity, but also covering a wider region of the potential behaviour space, 

The extension of the methodologies of multi-vector trust into the marine space are already demonstrated, however including information from physical observations of actors in a network enables the detection and identification of a much wider range of behaviours.
We also demonstrate a method for assessing trust metrics in harsh environments in terms of their relative significance, and a method for establishing classification signatures for misbehaviours.

It is to be noted that this presented method is significantly more computationally intensive than the relatively simple Hermes / OTMF algorithms communications only algorithms, and is exponential in complexity as metrics and/or domains are added. The repeated metric re-weighting required for real time behaviour detection is therefore an area that requires optimization. More work needs to be done to characterise how worthwhile this approach is compared to a separate synthesis approach where by MTFM-style trust is generated and assessed on a per-domain basis and subsequently fuzed.

For greater fidelity and more optimal results, a wider range of weights can be used in the initial regression step; however this is computationally expensive given that weighting is applied to each perspective (i.e.\ observer/target node pair) for each trust assessment time step, presenting 15 perspectives at each time interval in the 6 node case.

Every effort has been made to avoid over-training the dataset, using cross validating sampling for regression and "best weight" generation, however more meta-analysis is required to further demonstrate the functionality of this process.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
