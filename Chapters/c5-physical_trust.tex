% !TeX spellcheck = en_GB
\def\ChapterTitle{Use of Physical Behaviours for Trust Assessment}

	\chapter{\ChapterTitle}
	\label{Chapter\thechapter}
	\lhead{Chapter \thechapter. \emph{\nameref{Chapter\thechapter}}} % Write in your own chapter title to set the page header

\section{Introduction}


However, these approaches to operational security have been totally focused on the establishment of trust/security in the communications domain, and ignore other potential threats to the network exploited through physical movement.
This threat is particularly evident in collaborative autonomous systems where nodes are tasked to accomplish some survey / exploration / observation objective in a distributed fashion, where individual nodes make decisions based on the actions of their ``team''. 
This collaboration opens the opportunity for a physically-misbehaving actor to selfishly conserve it's own resources, or maliciously ``drain'' a given target node.
Current security / trust systems applied to MANETs are not concerned with the threat of such physical misbehaviours.

In Section~\ref{sec:mobility}, we review the current use cases, deployments and mobility patterns of collaborative AUV operations, and the state-of-the-art in underwater localisation techniques.
In Section~\ref{sec:tmfs}, we discuss the use of Trust Management Frameworks and their relevance and applicability to marine operations.\todo{Probably do away with this, repeated in a few other places} 
In Section~\ref{sec:physbev}, we propose a collection of metrics to characterise the physical behaviours of node, and establish a set of physical ``misbehaviours'' to test these against.
In Section~\ref{sec:sim_and_valid}, we design a series of simulations, and tests to assess the detection and identification capabilities of three potential physical metrics for trust assessment.

\section{AUV Mobility and Localisation}\label{sec:mobility}

The use and applications of Autonomous Underwater Vehicles (AUVs) has undergone a great expansion in recent years; current applications and considerations are summarised below.

\subsection{AUV operations and deployments}

\subsubsection{Hydrographic Survey}

The use of AUVs in the place of manned-surface platforms or tethered undersea platforms enables greatly increased spatial and temporal sampling.
Importantly, the separation of AUVs from the noisy sea surface enables much more efficient survey operations.
This is particularly important when comparing to classical tow-line based measurements; where the mobility of the AUVs enables for much tighter-turning survey patterns or operation in inaccessible or hard-to-reach locations such as polar survey\cite{Curtin1993}.

Another significant factor is cost; the daily cost of operating a manned vessel can be considerably higher than the costs of deploying, operating and recovering one or more AUVs with equivalent capabilities\cite{Nicholson2008}.
Additionally, the use of low-power ``glider'' AUVs has lowered the barrier to entry for extended mission types, such as persistent environmental survey, or open-ocean operations. 
Depth-hardened AUVs have also opened up the deepest parts of the oceans to exploration, with the onboard autonomy, imagery and Simultaneous Location and Mapping (SLAM) techniques allowing deep-dwelling survey AUVs to react to bottom-surface features without the need for a tight craft-to-surface control loop.
The natural extension of these kind of applications is the use of AUVs on ice-covered planets such as Europa, where three-dimensional, autonomous navigation without an on-the-loop controller is vital for mission resource efficiency and success.

\subsubsection{Hull and Infrastructure Inspection}
Ongoing concerns regarding the security, safety and legality of international shipping has driven the application of AUVs to the area of near-surface hull and infrastructure inspections, looking for damage as well as devices such as limpet mines and other contraband.
This use case puts a range of unique pressures on the AUV system; requiring highly accurate three-dimensional localisation and path-planning to clearly image the contours of a hull\cite{Nicholson2008}.
Similarly, with the increasing use and criticality of intercontinental undersea optical fibre connections, using AUVs for both the laying of and inspection of these cables is an exciting area of work\cite{Yu2004}\cite{Asakawa2002}.

\subsubsection{Marine Petrochemical}
Oil and Gas industry requirements for high quality, low altitude bathymetry of seabed structures for infrastructure development (pipelines/drill platforms etc.) as well as monitoring of those structures over time (inspection etc.) is another significant application area, and a major driver of research investment.
As in Hydrography, the mobility of AUVs is the biggest single advantage over classical platforms\cite{Morr2003}.

\subsubsection{Military}
Mine-Countermeasure Operations benefit greatly from, and significantly drive, AUV development; the ability to rapidly explore and covertly survey a potentially dangerous area without risking a human operator is a major benefit.
This benefit applies to protection as well as incursion; the ability to have persistent survey of a valuable area such as a forward-operating harbour is increasingly essential, and as AUV technology, autonomy and security practices develop, this use is increasing.
This Port Protection capability is particularly complex;  teams of AUVs are expected to repeatedly survey an area and remain densely-connected enough to maintain end-to-end communications with all other nodes, in the face of an environment that is possibly not well surveyed initially, and includes dynamically moving obstacles (i.e.\ ships).
In Sec.~\ref{sec:sim_and_valid}, we use this Port Protection scenario as a baseline for our simplified simulation context.

\todo{Look at redoing this with other mobilities (particularly distributed lawnmower)}

\subsection{Localisation Technologies}

Given the subsurface nature of most AUV operations, terrestrial localisation techniques such as GPS are unavailable (below $\approx 20cm$ depth). 
However, a range of alternative techniques are used to maintain spacial awareness to a high degree of accuracy in the underwater environment.
\subsubsection{Long baseline (LBL)}
Long-baseline localisation systems use a series of static surface/cable networked acoustic transponders to provide coordinated beacons and (usually) GPS-backed relative location information to local subsurface users. 
Such systems can be accurate to less that $0.1m$ or better in ideal deployments and are regularly used in controlled autonomous survey environments such as harbour patrol operations where the deployment area is bounded. 
However, the initial setup and deployment required in advance of any AUV operation makes LBL difficult to utilise in unbounded or contended areas.
LBL systems can also be deployed on mobile surface platforms in the area (ships or buoys for example), but these applications put significant computational pressure on the end-point AUV and have greatly reduced accuracy compared to ideal deployments\cite{Matos1999}.
\subsubsection{Doppler Velocity Log (DVL)}
Doppler Velocity Logging involves the emission of directed acoustic ``pings'' that reflect off sea bed/surface interfaces that, when received back on the craft with multi-beam phased array acoustic transducers can measure both the absolute depth/altitude (z-axis) of the craft and through directional Doppler shifting, the relative (xy-translative) motion of the craft since the ping.
While classical DVL was highly sensitive to shifting currents in the water column, advances in the development of Acoustic Doppler Current Profiling has turned that situation on it's head, enabling the compensation-for and measurement-of water currents down to the sub-meter level\cite{Snyder2010}.
\subsubsection{Inertial Navigation Systems (INS)}
Inertial navigation systems use gyroscopic procession to observe the relative acceleration of a mobile platform.
This reference-relative monitoring is particularly useful in the underwater environment, as it detects the motion of AUVs as they are carried by the water itself.
Bias Drift is a significant problem for INS systems operating over longer (hundreds of metres) distances, as they usually have some minimal amount of directional bias, that incurs a cumulative effect over time without assistance.
Several sensor synthesis processes have been demonstrated which combine information from INS along with DVL data to improve localisation into the sub-decimeter level\cite{Jalving2003}\cite{Liu2014}.
\subsubsection{Simultaneous Location and Mapping (SLAM)}
Simultaneous Location and Mapping is the process of iteratively developing a feature-based model of an environment, and to use the relative movement within that modeled environment to obtain estimates of absolute positioning.
SLAM has been most well developed in the contexts of either visual-based inspection using cameras, or LIDAR-style distance triangulation, however the same principles have been successfully applied using marine sonar readings, providing sub-meter accuracy, real-time, feature-relative localisation information that is (for the most part) environmentally agnostic\cite{Williams2000}.

In summary, current technology reliably enables AUVs to localise to a sub-metre accuracy in most areas of application.

\section{Trust Management Frameworks}\label{sec:tmfs}

Trust Management Frameworks (TMFs) provide information to assist the estimation of future states and actions of nodes operating as teams, groups or networks. 
This information is used to optimize the performance of a team against malicious, selfish, or defective misbehaviour by one or more nodes.
Previous research has established the advantages of implementing communications-based TMFs in terrestrial, 802.11 based MANETs, particularly in terms of preventing selfish operation in collaborative systems \cite{Li2007}, and maintaining throughput in the presence of malicious actors \cite{Buchegger2002}. 
These observations then inform future decisions of individual nodes, for example, route selection \cite{Li2008}.

Recent work has demonstrated the use of a number of metrics to form a ``vector'' of trust. The Multi-parameter Trust Framework for MANETs (MTFM) \cite{Guo2012}, uses a range of communications metrics beyond packet delivery/loss rate (PLR) to assess trust. This vectorized trust also allows a system to detect and identify the tactics being used to undermine or subvert trust. 
This method as been previously applied to the marine space, comparing against a selection of existing communications TMFs \cite{Bolster2015} showing that MTFM is more effective at detecting misbehaviours in sparse communications environments. 

\section{Physical Behaviours for Trust}\label{sec:physbev}

\subsection{Physical Metrics}

Three physical metrics are used to encompass the relative distributions and activities of nodes within the network; Inter-node Distance Deviation (INDD), Inter-node Heading Deviation (INHD), and Node Speed. 
Conceptually, INDD is a measure of the average spacing of an observed node with respect to its neighbours. 
INHD is a similar approach with respect to node orientation.
As such, these metrics completely encapsulate and abstract the physical behaviour of any node, potentially performing any misbehaviour.
Given that local nodes within the team are aware of the reported positions and velocities of their neighbours, it is believed that this is a reasonable initial set of metrics to establish the usefulness of physical metrics of trust assessment.

Additional metric constructions may be more suitable for certain contexts, platforms or operations, however these were selected in collaboration with UK DSTL and NATO CMRE as suitable, generic, assessments, viable on most current platforms in most current deployment schemes.

\begin{align}
  INDD_{i,j} &= \frac{|P_j - \sum_x \frac{P_x}{N}|}{\frac{1}{N}\sum_x \sum_y{|P_x - P_y| (\forall x \neq y)}}\\
  INHD_{i,j} &= \hat{v} \vert v= V_j - \sum_x{\frac{V_x}{N}}\\
  V_{i,j} &= |V_j|
\end{align}

Where $i$ and $j$ are indices denoting the current observer node and the current observed node respectively; $x$ is a summation index representing other nodes in the observers region of concern; $P{j}$ is the $[x,y,z]$ absolute position of the observed node (relative to some coordinated origin point agreed upon at launch) and $V{j}$ is the $[x,y,z]$ velocity of the observed node.

Thus, the metric vector used for the physical-trust assessment from one observer node to a given target node is;

\begin{equation}
  X_{i,j}=\{\text{INDD}_{i,j}, \text{INHD}_{i,j},, V_{i,j}\}
  \label{eq:phys_vector}
\end{equation}
At each time-step, each node will have a separate $X$ assessment vector for each node it has observed in that time. 
Ergo the fleet or team as a whole will have $N\times N$ assessment vectors at each timestep.

\subsection{Physical Misbehaviours}

Misbehaviours in the communications space is heavily investigated area in MANETs \cite{Konate2011}\cite{Wang2009}\cite{Chen2014a}\cite{Mitchell2014}, but attacks and misbehaviours in the physical space are far less explored. 
Both in terrestrial and underwater contexts, as MANET applications expand and become increasingly \emph{de rigueur}, the impacts of physical or operational misbehaviour become increasingly relevant. 
As in the communications space, the primary drivers of any ``misbehaviour'' come under two general categories; selfish operation or malicious subterfuge.
Autonomous MANETs in general rely (or are at least, most effective) when all nodes operate fairly, be that in terms of their bandwidth sharing, energy usage, routing optimality or other factors. 
Physically, if a node is being ``selfish'', it may preferentially move to the edge of a network to minimise it's dynamic work allocation, or depending on it's intent, may insert itself into the centre of a network to maximise it's ability to capture, monitor, and manipulate traffic going across the network. 
In the context of a secure operation (or one that's assumed to be secure), the opportunity for capturing a legitimate node and replacing it with a modified clone.
Assuming a highly capable outside actor and a multi-channel communications opportunity, there is also the possibility of a node appearing to ``play along'' with the crowd that occasionally breaks rank to route internal transmissions to a outside agent.
In the underwater context this may mean an AUV following the rest of a team along a survey path and occasionally ``breaking surface'' to communicate to a malicious controller.
Alternatively, if an inserted node is not totally aware of a given mission parameter, such as a particular survey or waypointing path, it may simply follow along, hoping not to be noticed.

In all these cases, such behaviour involves some element of behaving differently from the rest of the team, however, there are other cases where such individual ``deviance'' is observed; where a node is in some kind of mechanical ``failure state''.
In the underwater context, this could be damage to the drive-train or navigation systems, causing it to lag behind or consistently drift off course. 
An ideal physical trust management system would be able to differentiate between both ``malicious'' behaviours and ``failing'' behaviours.

To investigate this hypothesis, we create two ``bad'' behaviours; one ``malicious'', where a cloned node is unaware of the missions' survey parameters and attempts to ``hide'' among the fleet, and a ``failing'' node, with an impaired drive train, increasing the drag force on the nodes movement.
These two behaviours are designated \emph{Shadow} and \emph{SlowCoach} respectively.

\section{Simulation and Validation}\label{sec:sim_and_valid}

\subsection{Simulation Background}

Simulations were conducted using a Python based simulation framework, SimPy \cite{Mueller2003SimPy}, with a network stack built upon AUVNetSim \cite{Miquel2008}, with transmission parameters taken from and validated against \cite{Stojanovic2007} and \cite{Stefanov2011}.
For the purposes of this chapter, this network is used for the dissemination of node location information, assuming suitable compression of internally assumed location data compressed into one 4096 bit acoustic data frame, with the network overall emitting approximately 10 frames a minute.
Node kinematics are modeled on REMUS 100 AUVs, based on limits and core characteristics given in \cite{Mcewen2001}, \cite{Milgram2001} and \cite{Samad2011}.\footnote{While the hydrodynamics of the control surfaces of the AUVs are not modeled in this case, axial drag is modeled as a resistive inertial force on the craft.}

These limits are given in Table~\ref{tab:mobility_sysconstraints}
\begin{table}[h]
  \caption{REMUS 100 Mobility Constraints as applied in simulation} \label{tab:mobility_sysconstraints}
  \begin{center}
    \setlength{\tabcolsep}{8pt}
    \begin{tabular}{lcc}
      \toprule
      Parameter & Unit & Value \\
      \midrule
      Length & $m$ & 5.5\\
      Diameter & $m$ & 0.5\\
      Mass & $kg$ & 37 \\ 
      Max Speed & $ms^{-1}$ & 2.5\\
      Cruising Speed & $ms^{-1}$ & 1.5\\
      Max X-axis Turn & $^{\circ} s^{-1}$ & 4.5\\
      Max Y-axis Turn & $^{\circ} s^{-1}$ & 4.5\\
      Max Z-axis Turn & $^{\circ} s^{-1}$ & 4.5\\
      Axial Drag Coefficient ($c_d$) & NA & 3\\
      Cross Section Area & $m^2$ & 0.13\\
      \bottomrule
    \end{tabular}
    \setlength{\tabcolsep}{6pt}
  \end{center}
\end{table}

\subsection{Node Control Modelling}

In our investigation, we use the example of a Port Protection scenario, where a team of six AUVs are tasked with surveying a simplified harbour; in this case a 1kmx1kmx100m cuboid volume.
This is accomplished through a distributed way point system where by the team overall must ``check'' several points around the exterior and interior of this volume in reasonable time.

This consists of three heuristic rules; Cohesion, Repulsion and Alignment.
\begin{align}
  F_{j,C}=& F_+\left(p_j, \frac{1}{N}\sum\limits_{\forall i \ne j}^N{p_i}, d_{max}\right)\label{eq:fa}\\
  F_{j,R}=& \sum\limits_{\forall i \ne j}^N F_-\left(p_j, p_i, d_{max}) \big| d_{max}>\|p_i-p_j\|\right)\label{eq:fr}\\
  F_{j,A}=& \frac{1}{N}\cdot\left(\sum\limits_{\forall i \ne j}^N \hat{v_i}\right)\label{eq:fc}
\end{align}
Where $F$'s are force-vectors applied to the internal guidance of the AUV, $F_{j,C}$ representing Cohesion, $F_{j,R}$ representing Repulsion, and $F_{j,A}$ as Alignment: $F_+$ is a scaled vector attraction function, and $F_-$ is an equivalent repulsion function
\begin{align}
  F_+(p^a, p^i)=&(\widehat{p^a-p^i}) \times \frac{|p^a-p^i|}{d}\\
  F_-(p^r, p^i)=&(\widehat{p^i-p^r}) \times \frac{|p^r-p^i|}{d}
\end{align}

\begin{figure*}
  \centering
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{flocking_cohesion}
    \caption{Cohesion}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{flocking_separation}
    \caption{Repulsion}
  \end{subfigure}
  \begin{subfigure}[t]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{flocking_alignment}
    \caption{Alignment}
  \end{subfigure}
  \caption{Visual representation of the basic Boidean collision avoidance rules used}
  \label{fig:boids}
\end{figure*}


\subsection{Standards of Accuracy}\label{sec:standards}

The key question of this chapter is to assess the advantages and disadvantages of utilising trust from the physical domain. 

It is important to clarify what is meant by ``effective'' in this case; the ``effectiveness'' of any trust assessment framework is taken as consisting of several parts, the \emph{accuracy} of detection and identification of a particular misbehaviour, the \emph{complexity} of such analysis, including any specific training required, and the \emph{differentiability} of behaviours using given metrics.

In this case we are particularly interested in the accuracy of detection and identification of malicious / failing behaviours, and as such are looking at three key characteristics of accuracy; true detection accuracy (what percentage of ``bad'' behaviours are detected at all); false positive rates (what percentage of ``control'' behaviours are detected as being ``bad''); and misidentification rates (how many instances of one bad behaviour are mischaracterised as the other and vice versa.

As such we have three primary questions to answer to establish if these metrics are useful: 
How accurate are these metrics in being able to easily differentiate between Normal and Abnormal behaviours in terms of True-Positive and False-Positive rates?
What differentiation of response, if any, is there between the stated abnormal behaviours?
Can a simple classification be built to characterise these differentiations of response, and what is it's True-Positive/False-Positive accuracy?


\subsection{Analysis}
Having established the metrics under investigation, 64 simulation runs are executed for each scenario (i.e.\ one node ``Maliciously'' following the fleet with no mission information (Shadow), one ``Failing'' node with simulated drive train issues (Shadow), and one baseline control scenario where all nodes are behaving appropriately (Control).
Each of these simulated missions last for an hour, matching realistic deployment times based on current MOD/NATO operations\cite{Bolster2014a}.

\subsubsection{Metric Cleaning}
In order to assess the viability of using the previously discussed metrics, the raw motion paths recorded by the simulation are fed into an analysis pipeline aimed at abstracting the instantaneous observed values into derived deviations from ``normal'' behaviour in the team.

\begin{align}
  d_{i,j}^{m,t} &= x_{i,j}^{m,t} - \frac{\sum_k x_{i,k}^{m,t}}{|M|}\label{eq:d}\\
  \alpha_{i,j}^{m,t} &= | \frac{d_{i,j}^{m,t}}{\sigma{d_{i,j}^{m,t}}}|\label{eq:dd}
\end{align}

Where $i$ and $j$ are indices denoting the current observer node and the current observed node respectively; $x$ is a summation index representing other nodes in the observers region of concern; $X$ is the vector of metrics from~\ref{eq:phys_vector}; $d$ is an intermediate value of the distance of a given observation from the mean, and $\alpha$ is a resulting normalised response value in terms of it's deviation from the mean.

\subsubsection{Behaviour Detection and Classification}
A simple misbehaviour detection is to apply Dixon's Q-test \cite{Dean1951} to the resultant $\sum\alpha$ values for each node for each metric for each run establishing if a ``misbehaving node'' exists in a given run, and if so, attempt to identify that misbehaving node. 
For our initial investigation we will use a Confidence Interval of $95\%$.

Our initial hypothesis is that by using observations of the previously stated physical metrics, that we will be able to detect and identify misbehaviours.
Within that context, this Confidence Interval indicates that we would expect only a $5\%$ chance that any run or node identified using the Q-test to \emph{not} be a misbehaving run/node.
Further, due to the range of metrics available, by applying the Q-test on a per-metric basis, we can use the ``votes'' of each metric as a simplified consensus classifier.
This classifier may allow us to characterise some aspect of a given misbehaviour in terms of metrics it heavily impacts, and those that are less affected, finding some differentiating-limit between certain behaviours using certain metrics.

\begin{equation}
  C_{i}^{m} = \Sigma_t\sigma_{i}^m * \frac{N-1}{\sum_{x\neq i}{\Sigma_t\sigma_{x}^m}}\label{eq:confidence}
\end{equation}

\subsubsection{Operational Performance Metrics}
While not the focus of this paper, we are also concerned with the impact of these misbehaviours on the mission efficiency of the team overall.
We monitor this in three main measurements; the ``speed'' of the fleet in terms of how many of it's port-protection way points it successfully approaches and passes, the total energy used for communications, and the average end-to-end delay in the acoustic network.
We would expect that any misbehaviour in positioning will incur some loss of efficiency, whether it is the fleet being slowed down by a straggler attempting to catch up or of a node moving in an unexpected fashion dragging the team temporarily off course.
Given that in acoustic communications, transmission is energetically expensive while reception is not, and while physical misbehaviours will not impact the amount of offered load on the network, collisions induced by uneven distribution of nodes should have a small but measurable effect on energy used for packet reception.

\section{Results and Discussion}
Fig.~\ref{fig:metric_values} shows the raw metric values (vertically) from one run of each behaviour (horizontally), starting with the Control case, where all node are behaving properly.
The misbehaving node in the remaining cases.
It clear that using the (unitless) INDD and INHD metrics, Alfa is the outlier and other, fairly behaving, nodes are all consistent in their metric values.
This outlier-response is not nearly as clear in the Speed metric case (bottom row of Fig.~\ref{fig:metric_values}).

From a behaviour-perspective, it appears that the Shadow behaviour is creating the largest, most obvious deviations.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{Metric_Values}
  \caption{Observed Metric Values for one simulation of each behaviour ($x_{i,j}^{m,t}$ from \eqref{eq:d})}
  \label{fig:metric_values}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{Metric_Deviation}
  \caption{\emph{Unnecessary but included for draft discussion} Observed Metric Values for one simulation of each behaviour ($d_{i,j}^{m,t}$ from Fig.~\ref{fig:workflow})}
\end{figure*}

In Fig.~\ref{fig:deviance_values} the metric values are normalised as per \eqref{eq:dd}.
This has highlighted the outlying-characteristic of INHD and INDD; largely eliminating the other nodes-responses.
In the Speed response of Fig.~\ref{fig:deviance_values}, the Speed metric is not obviously highlighting any significant misbehaviours in that metric. 

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{Metric_Sigma_Deviance}
  \caption{Normalised Deviance values from one simulation of each behaviour ($\alpha_{i,j}^{m,t}$ from \eqref{eq:dd})}
  \label{fig:deviance_values}
\end{figure*}

From Fig.~\ref{fig:summedsigmabar}, it appears that Speed is being significantly affected by the differing behaviours, but much less so than INHD/INDD.  

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{summedsigmabar}
  \caption{Per-Node-Per-Run deviance for each metric, normalised in time ($\sum\alpha/T$)}
  \label{fig:summedsigmabar}
\end{figure*}

\subsection{Detection of Misbehaviours}
We have demonstrated by graphical result that from our initial metrics, INHD and INDD do appear to accurately and obviously identify the malicious node in the case that there is one. 
Using the deviance normalisation presented in \eqref{eq:dd}, clear, almost contiguous areas under the Alfa-values are observed in Fig.~\ref{fig:deviance_values} in the Shadow and SlowCoach misbehaviours.
Further, from Fig.~\ref{fig:summedsigmabar}, it is shown that while it is nowhere near as ``clear'' as the deviance in INHD and INDD, that the Speed metric is still registering a statistically significant deviation in both misbehaviours, and that the difference between the deviances in Speed may indicate a way to analytically differentiate between the two misbehaviours.

To investigate how this would relate to the ability to blindly detect misbehaviours, the Q-test is applied to $\Sigma\alpha$ results as used in Fig.~\ref{fig:summedsigmabar}, to attempt to correctly establish:
\begin{enumerate}
  \item if a node is misbehaving and
  \item which node is misbehaving
\end{enumerate}

As such, the ``correctness'' rule for assessing this strategy is that, in misbehaving cases, the Q-tests should return Alfa (otherwise a ``Fail'' is recorded), and in the Control case, the Q-test should assert that there are no obvious outliers, (otherwise a ``Fail'' is recorded again).
In Table~\ref{tab:overall_stats}, the Null case (Control behaviour) is correctly identified 92\% of the time.
The ``malicious'', Shadow misbehaviour is detected and identified 98\% of the time, and the ``failing'', SlowCoach misbehaviour is identified just 79\% of the time. 
These values match our intuition from Figs.~\ref{fig:metric_values} and~\ref{fig:deviance_values}.

\begin{table}
  \caption{Overall Q-Test Outlier Correct Detection Accuracy}
  \centering
\begin{tabular}{lll}
\toprule
Behaviour &  Mean &   Std \\
\midrule
Control   & 0.927 & 0.261 \\
Shadow    & 0.979 & 0.144 \\
SlowCoach & 0.792 & 0.408 \\
\bottomrule
\end{tabular}
  %\input{figures/overall_stats.tex}
  \label{tab:overall_stats}
\end{table}

We can investigate this further by looking at the ``correctness'' of the assessments of each metric individually (Table~\ref{tab:per_metric_stats}).
In both misbehaviours, INHD and INDD correctly identify Alfa as the misbehaver 100\% of the time. 
However, they misidentify a potential misbehaviour in the Control case 13\% and 7\% of the time respectively.
Meanwhile, Speed correctly identified the Control case 97\% of the time, and the Shadow case 94\% of the time, but missed the SlowCoach behaviour 63\% of the time. 
This result is surprising on the face of it, as SlowCoach is a misbehaviour that is exclusively about individual node speed and conceptually should have had a much larger impact on the simple Speed metric.
However, the collaborative nature of the collision avoidance system, and the existing limits on node kinematics from Table~\ref{tab:mobility_sysconstraints} appear to be hiding this impact. 

\begin{table}
  \caption{Per-Metric Q-Test Outlier Detection Accuracy}
  \centering
\begin{tabular}{lllll}
\toprule
{} & Behaviour &  INDD &  INHD & Speed \\
\midrule
Mean & Control & 0.875 & 0.938 & 0.969 \\
     & Shadow & 1.000 & 1.000 & 0.938 \\
     & SlowCoach & 1.000 & 1.000 & 0.375 \\
Std & Control & 0.336 & 0.246 & 0.177 \\
     & Shadow & 0.000 & 0.000 & 0.246 \\
     & SlowCoach & 0.000 & 0.000 & 0.492 \\
\bottomrule
\end{tabular}
  %\input{figures/per_metric_stats.tex}
  \label{tab:per_metric_stats}
\end{table}

\subsection{Identification of Misbehaviours}
Having established the ability of INDD, INHD and Speed to all detect physical misbehaviour to a statistically significant level, and having shown that there is a demonstrable difference in response to different misbehaviours, we return to the last question from Sec.~\ref{sec:standards}; can a simple classifier based on a subset of our results be constructed, and can it be blindly applied to a new set of results successfully?

From~\eqref{eq:confidence}, the per-metric-per-behaviour ``Confidence'' in the relationship between a given metric deviance and each behaviour is established. It is hypothesised that this confidence can be used as a signature for that metric.

%\begin{table}[h]
%  \caption{Metric Confidence Responses for known behaviours~\eqref{eq:confidence}}
%  \centering
%\begin{tabular}{lrrr}
%\toprule
%metric &  INDD &  INHD &  Speed \\
%var       &       &       &        \\
%\midrule
%Control   & 0.970 & 0.914 &  0.892 \\
%Shadow    & 4.459 & 3.875 &  1.797 \\
%SlowCoach & 3.910 & 2.847 &  1.518 \\
%\bottomrule
%\end{tabular}
%\end{table}
\begin{table}[h]
  \caption{Metric Confidence Responses for known behaviours~\eqref{eq:confidence}}
  \centering
\begin{tabular}{llrrr}
\toprule
{} & Behaviour &  INDD &  INHD &  Speed \\
\midrule
Mean & Control & 1.064 & 0.966 &  1.010 \\
     & Shadow & 4.059 & 3.374 &  2.098 \\
     & SlowCoach & 4.246 & 3.352 &  1.491 \\
Std & Control & 0.262 & 0.113 &  0.132 \\
     & Shadow & 0.398 & 0.436 &  0.206 \\
     & SlowCoach & 0.198 & 0.288 &  0.180 \\
\bottomrule
\end{tabular}
  %\input{figures/confidence.tex}
  \label{tab:confidence}
\end{table}

Having demonstrated that the Null case (All nodes behaving fairly) can be identified to a strong degree of accuracy, our classifier will continue to use the Q-test across all metrics for that case and concentrate of differentiating the Shadow and SlowCoach behaviours where they exist.

From Table~\ref{tab:confidence} it is clear that INHD and INDD have similar responses to both misbehaviours, with significant standard deviations, but the response of the Speed metric is much more stable and discernible; across the range of training simulation runs.
In the SlowCoach behaviour, this Speed response centres around $1.5$, while the Shadow behaviour centres around $2.0$, with these centres being at least one standard deviation away from each other respectively.

Our generated classifier is formalised in~\eqref{eq:classifier}.

\begin{equation}
  C \rightarrow 
  \begin{cases}
    % Control
    Q^{95}(X) = \emptyset,& \text{Control}\\
    % Shadow
    Q^{95}(X) \neq \emptyset \land \text{Speed}^X \leq 1.75, & \text{Shadow}\\
    % SlowCoach
    Q^{95}(X) \neq \emptyset \land \text{Speed}^X > 1.75,& \text{SlowCoach}\\
  \end{cases}
  \label{eq:classifier}
\end{equation}

Applying this simplified classifier to a blind test set of simulations (of the same scale) gives surprisingly positive results as shown in Table~\ref{tab:classifier}, with greater than 90\% identification rates for both misbehaviours.
However, in the Null (Control) case we experience a false-positive rate of nearly 30\%, that is to say that in the case where there is no misbehaviour, 30\% of the time a node will be misidentified as misbehaving when it is not.

Given the simplicity of the applied classifier, these are strongly positive results for the use of physical metrics for behaviour discrimination; with INHD and INDD proving as strong and obvious ``canaries'' of misbehaviour, and Speed in this case proving a capable differentiator between conceptually close misbehaviours.

\begin{table}[h]
  \caption{Successful Identification rates on untrained results using~\eqref{eq:classifier}}
  \centering
  \begin{tabular}{lr}
    \toprule
    True Behaviour &  Probability of Correct Blind Identification \\
    \midrule
    Control        &                                        0.719 \\
    Shadow         &                                        0.906 \\
    SlowCoach      &                                        0.938 \\
    \bottomrule
  \end{tabular}
  %\input{figures/classifier.tex}
  \label{tab:classifier}
\end{table}

\begin{table}[h]
  \caption{Successful Identification rates on untrained results using~\eqref{eq:classifier}, with outlier consensus checks}
  \centering
  \input{Figures/input/classifier_minority.tex}
  \label{tab:classifier_minority}
\end{table}
\todo{Explain the Minority Classifier}



\subsection{Impacts of Misbehaviour on operational performance}
The anticipated ``small but measurable'' effects to communications performance and energy usage are indeed extremely small and within the bounds of statistical uncertainty.
One observation of merit was an observed 10\% increase in end-to-end delay in the case of the Shadow behaviour; this is due to the misbehaving node ``overshooting'' the mission way points and thus temporarily looking local connection to nodes on the opposite side of the fleet from it, causing retransmissions thus, delays.
As for physical efficiency, achievement rates were identical to within 2\% error on each run across all behaviours, and fleet distance varied by a similar margin.
It's possible that our selected behaviours were too unambitious in our impacts, and future work will have to investigate the impact of ``heavy-handed'' or destructive behaviours on the operational efficiency of autonomous networks.

\section{Conclusion}
In this paper we have demonstrated that with current and on-the-horizon underwater localisation techniques, that in certain mobility models, that a set of relatively simple geometric abstractions (INHD, INDD, and Speed), between nodes as part of an Underwater MANET can be used as a Trust Assessment and Establishment metric.

These metrics are application-agnostic and could potentially be applied in other areas of mobile autonomy such as UAV operations and Autonomous Vehicular Networks.

We show, using a Port-Protection way point scenario built upon a Boidian collision prevention behaviour that in a simulated underwater environment, the outputs of these metrics can be used to detect and differentiate between exemplar malicious behaviour and potential failure states.

This verification further supports the assertions the authors have made previously that it is practical to extend Trust protocols such as Multi-parameter Trust Framework for MANETS (MTFM)\cite{Guo2012} to include metrics and observations from the physical domain as well as those from the communication domain\cite{Bolster2014}.
This combination of physical and ``logical'' information would further support the decentralised and distributed establishment of observation based Trust.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
